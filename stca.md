# 推荐算法(2026.2.26)：字节，超长序列建模，完播率+3%

关注我，每天为你精挑细选最优质、最新鲜的推荐算法paper，陪你一起保持进步、不断精进！

### 论文：Make It Long, Keep It Fast: End-to-End 10k-Sequence Modeling at Billion Scale on Douyin
### 网址：https://arxiv.org/pdf/2511.06077
### 公司：字节
### 思想：堆叠
### 方向：超长行为序列建模

## 解读：
提出了一个长行为序列建模的方法。放弃了self-attention，只用target attention，通过所谓的堆叠来增大网络，以提高建模效果。将建模的用户表征接入主网络做用户的各类行为的概率预估。

### （1）STCA——堆叠target attention
为了让模型更大，堆叠多层target attention，从而希望模型有scaling laws的效果增加。

不是传统的堆叠，这一层的输出作为下一层的输入，而是将target attention层获得用户兴趣表征，跟target item的embed一起生成新的query，用于本层的attention计算。

我画了一个简单的示意图。左小角是行为序列；右侧的绿色是candidate item（用作query）。某层的query是拼接了之前层的attention计算的用户兴趣表征和原始的candidate item表征，再接简单网络的结果。为了简单期间，只标记了第3层的query的数据来源。

STCA最后的输出表征是所有层的用户兴趣表征的拼接。

![alt text](imgs/stca.png)

### （2）请求级批处理
这是训练时候的技巧。实际上，就是HSTU里的多candidate预测。 一个样本只有一个candidate item，而用户的一次请求却有多个样本，它们都对应相同的用户行为序列。

本文将本次请求的多个candidate（注意不是所有）放到一个batch里进行处理，从而减少传输量，也能一定程度上减少计算量。

### （3）随机采样行为序列
最早是HSTU就是这样做的，这样的目的和好处是节省传输和计算成本。效果肯定会打折扣，希望以微小的效果降低，换得巨大的算力降低，从而让长序列可以落地生产环境。

就是训练的时候对行为序列做随机采样，预估时候用10k长的序列。本文起了一个优雅的说法——Train Sparsely, Infer Densely。这里的 sparse / dense 指的不是模型参数，而是每条训练/推理样本里实际处理的序列长度（token 数量）。

发现，全10k训练的80%收益折扣，却减少了2/3算力。可见，这个还是很值的。

**AB**： 一个月，完播率大幅提升3.3%~4.2%，同时 App 停留时长、评论、点赞等核心指标全面正向，低中活跃用户收益最明显。

## 心得：
* target attention之后获得一个表征，按照传统的堆叠方式，是无法实现堆叠的，也就是在key和value上无法做文章，只好在query上做堆叠。这也是没人做堆叠的原因，本文想出来的堆叠方法很特别。
* 长行为序列建模，做一层target attention效果是不行的，或者是不明显的，必须想办法堆叠。

## 愚见
self-attention是不能完全舍弃的，还是要一定的方式保留，比如对短序列按照传统方式（self-attention+target attention）处理。论文为了体现自己的方法，不说，也是可以理解的。

## 可信度：生产

## 推荐等级：有实践价值

![pic](./imgs/stca.png)

**请帮忙点赞、转发，谢谢。欢迎干货投稿 \ 论文宣传\ 合作交流**


### 【铁粉】请入微信群，群内我会给出更深入的解读，还可以共同讨论技术方案、发招聘广告、内推和交友等。
* 铁粉标准：关注公众号一个月以上，且在公众号上累计15次互动（评论、爱心、转发）、或投稿1次、或打赏199，只欢迎技术同学。
* 入群方法：请您加个人微信lmxhappy，我拉您入群，请备注【公司】（只我个人看，不公开）。

## 推荐您继续阅读：


